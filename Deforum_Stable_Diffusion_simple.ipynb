{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markusstrasser/deforum/blob/main/Deforum_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c442uQJ_gUgy"
   },
   "source": [
    "# **Deforum Stable Diffusion v0.5**\n",
    "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n",
    "\n",
    "Notebook by [deforum](https://discord.gg/upmXXsrwZc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBamKxcmNI7-"
   },
   "source": [
    "By using this Notebook, you agree to the following Terms of Use, and license:\n",
    "\n",
    "**Stablity.AI Model Terms of Use**\n",
    "\n",
    "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
    "\n",
    "The CreativeML OpenRAIL License specifies:\n",
    "\n",
    "You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
    "CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
    "You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
    "\n",
    "\n",
    "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4knibRpAQ06"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "2g-f7cQmf2Nt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A4000, 16376 MiB, 16116 MiB\n",
      "\n",
      "Local Path Variables:\n",
      "\n",
      "models_path: /content/models\n",
      "output_path: /content/output\n",
      "Setting up environment...\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Cloning into 'stable-diffusion'...\n",
      "remote: Enumerating objects: 1369, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 1369 (delta 12), reused 22 (delta 12), pack-reused 1344\u001b[K\n",
      "Receiving objects: 100% (1369/1369), 70.16 MiB | 39.28 MiB/s, done.\n",
      "Resolving deltas: 100% (737/737), done.\n",
      "  Running command git clone -q https://github.com/CompVis/taming-transformers.git /workspace/deforum/src/taming-transformers\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /workspace/deforum/src/clip\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Cloning into 'AdaBins'...\n",
      "remote: Enumerating objects: 80, done.\u001b[K\n",
      "remote: Total 80 (delta 0), reused 0 (delta 0), pack-reused 80\u001b[K\n",
      "Unpacking objects: 100% (80/80), done.\n",
      "Cloning into 'MiDaS'...\n",
      "remote: Enumerating objects: 501, done.\u001b[K\n",
      "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 501 (delta 69), reused 46 (delta 44), pack-reused 409\u001b[K\n",
      "Receiving objects: 100% (501/501), 414.88 KiB | 3.24 MiB/s, done.\n",
      "Resolving deltas: 100% (167/167), done.\n",
      "Cloning into 'pytorch3d-lite'...\n",
      "remote: Enumerating objects: 55, done.\u001b[K\n",
      "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 55 (delta 30), reused 38 (delta 15), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (55/55), done.\n",
      "Cloning into 'k-diffusion'...\n",
      "remote: Enumerating objects: 427, done.\u001b[K\n",
      "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 427 (delta 130), reused 124 (delta 124), pack-reused 281\u001b[K\n",
      "Receiving objects: 100% (427/427), 76.34 KiB | 1.29 MiB/s, done.\n",
      "Resolving deltas: 100% (286/286), done.\n",
      "\n",
      "Environment set up in 153 seconds\n"
     ]
    }
   ],
   "source": [
    "SETUP = True\n",
    "\n",
    "if SETUP:\n",
    "    !python setup.py\n",
    "    !pip install gdown\n",
    "    !gdown https://drive.google.com/uc?id=1N06uaEC-5c1ciC_CVwqfhD-IPbOtCVOa\n",
    "    !mv sd-v1-4-full-ema.ckpt ../models/model.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov3r4RD1tzsT"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j7rgxvLvfay"
   },
   "source": [
    "### Animation Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63UOJvU3xdPS"
   },
   "source": [
    "### Prompts\n",
    "`animation_mode: None` batches on list of *prompts*. `animation_mode: 2D` uses *animation_prompts* key frame sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ujwkGZTcGev"
   },
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\n",
    "    \"a beautiful forest by Asher Brown Durand, trending on Artstation\", # the first prompt I want\n",
    "    \"a beautiful portrait of a woman by Artgerm, trending on Artstation\", # the second prompt I want\n",
    "    #\"this prompt I don't want it I commented it out\",\n",
    "    #\"a nousr robot, trending on Artstation\", # use \"nousr robot\" with the robot diffusion model (see model_checkpoint setting)\n",
    "    #\"touhou 1girl komeiji_koishi portrait, green hair\", # waifu diffusion prompts can use danbooru tag groups (see model_checkpoint)\n",
    "    #\"this prompt has weights if prompt weighting enabled:2 can also do negative:-2\", # (see prompt_weighting)\n",
    "]\n",
    "\n",
    "animation_prompts = {\n",
    "    0: \"a beautiful apple, trending on Artstation\",\n",
    "    20: \"a beautiful banana, trending on Artstation\",\n",
    "    30: \"a beautiful coconut, trending on Artstation\",\n",
    "    40: \"a beautiful durian, trending on Artstation\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.9 MB 280 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1844/796582547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeforum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrender_input_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_animation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_interpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_image_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/deforum_2/deforum.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/cv2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0mbootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/cv2/__init__.py\u001b[0m in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mpy_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mnative_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from deforum import render_input_video, render_animation, render_interpolation, render_image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8RAo2zI-vQm"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qH74gBWDd2oq"
   },
   "outputs": [],
   "source": [
    "\n",
    "if override_settings_with_file:\n",
    "    print(f\"reading custom settings from {custom_settings_file}\")\n",
    "    if not os.path.isfile(custom_settings_file):\n",
    "        print('The custom settings file does not exist. The in-notebook settings will be used instead')\n",
    "    else:\n",
    "        with open(custom_settings_file, \"r\") as f:\n",
    "            jdata = json.loads(f.read())\n",
    "            animation_prompts = jdata[\"prompts\"]\n",
    "            for i, k in enumerate(args_dict):\n",
    "                if k in jdata:\n",
    "                    args_dict[k] = jdata[k]\n",
    "                else:\n",
    "                    print(f\"key {k} doesn't exist in the custom settings data! using the default value of {args_dict[k]}\")\n",
    "            for i, k in enumerate(anim_args_dict):\n",
    "                if k in jdata:\n",
    "                    anim_args_dict[k] = jdata[k]\n",
    "                else:\n",
    "                    print(f\"key {k} doesn't exist in the custom settings data! using the default value of {anim_args_dict[k]}\")\n",
    "            print(args_dict)\n",
    "            print(anim_args_dict)\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "anim_args = SimpleNamespace(**anim_args_dict)\n",
    "\n",
    "args.timestring = time.strftime('%Y%m%d%H%M%S')\n",
    "args.strength = max(0.0, min(1.0, args.strength))\n",
    "\n",
    "if args.seed == -1:\n",
    "    args.seed = random.randint(0, 2**32 - 1)\n",
    "if not args.use_init:\n",
    "    args.init_image = None\n",
    "if args.sampler == 'plms' and (args.use_init or anim_args.animation_mode != 'None'):\n",
    "    print(f\"Init images aren't supported with PLMS yet, switching to KLMS\")\n",
    "    args.sampler = 'klms'\n",
    "if args.sampler != 'ddim':\n",
    "    args.ddim_eta = 0\n",
    "\n",
    "if anim_args.animation_mode == 'None':\n",
    "    anim_args.max_frames = 1\n",
    "elif anim_args.animation_mode == 'Video Input':\n",
    "    args.use_init = True\n",
    "\n",
    "# clean up unused memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# dispatch to appropriate renderer\n",
    "if anim_args.animation_mode == '2D' or anim_args.animation_mode == '3D':\n",
    "    render_animation(args, anim_args)\n",
    "elif anim_args.animation_mode == 'Video Input':\n",
    "    render_input_video(args, anim_args)\n",
    "elif anim_args.animation_mode == 'Interpolation':\n",
    "    render_interpolation(args, anim_args)\n",
    "else:\n",
    "    render_image_batch(args)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zV0J_YbMCTx"
   },
   "source": [
    "# Create video from frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "no2jP8HTMBM0"
   },
   "outputs": [],
   "source": [
    "skip_video_for_run_all = True #@param {type: 'boolean'}\n",
    "fps = 12 #@param {type:\"number\"}\n",
    "#@markdown **Manual Settings**\n",
    "use_manual_settings = False #@param {type:\"boolean\"}\n",
    "image_path = \"/content/drive/MyDrive/AI/StableDiffusion/2022-09/20220903000939_%05d.png\" #@param {type:\"string\"}\n",
    "mp4_path = \"/content/drive/MyDrive/AI/StableDiffu'/content/drive/MyDrive/AI/StableDiffusion/2022-09/sion/2022-09/20220903000939.mp4\" #@param {type:\"string\"}\n",
    "render_steps = True  #@param {type: 'boolean'}\n",
    "path_name_modifier = \"x0_pred\" #@param [\"x0_pred\",\"x\"]\n",
    "\n",
    "\n",
    "if skip_video_for_run_all == True:\n",
    "    print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
    "else:\n",
    "    import os\n",
    "    import subprocess\n",
    "    from base64 import b64encode\n",
    "\n",
    "    print(f\"{image_path} -> {mp4_path}\")\n",
    "\n",
    "    if use_manual_settings:\n",
    "        max_frames = \"200\" #@param {type:\"string\"}\n",
    "    else:\n",
    "        if render_steps: # render steps from a single image\n",
    "            fname = f\"{path_name_modifier}_%05d.png\"\n",
    "            all_step_dirs = [os.path.join(args.outdir, d) for d in os.listdir(args.outdir) if os.path.isdir(os.path.join(args.outdir,d))]\n",
    "            newest_dir = max(all_step_dirs, key=os.path.getmtime)\n",
    "            image_path = os.path.join(newest_dir, fname)\n",
    "            print(f\"Reading images from {image_path}\")\n",
    "            mp4_path = os.path.join(newest_dir, f\"{args.timestring}_{path_name_modifier}.mp4\")\n",
    "            max_frames = str(args.steps)\n",
    "        else: # render images for a video\n",
    "            image_path = os.path.join(args.outdir, f\"{args.timestring}_%05d.png\")\n",
    "            mp4_path = os.path.join(args.outdir, f\"{args.timestring}.mp4\")\n",
    "            max_frames = str(anim_args.max_frames)\n",
    "\n",
    "    # make video\n",
    "    cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-vcodec', 'png',\n",
    "        '-r', str(fps),\n",
    "        '-start_number', str(0),\n",
    "        '-i', image_path,\n",
    "        '-frames:v', max_frames,\n",
    "        '-c:v', 'libx264',\n",
    "        '-vf',\n",
    "        f'fps={fps}',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        '-crf', '17',\n",
    "        '-preset', 'veryfast',\n",
    "        '-pattern_type', 'sequence',\n",
    "        mp4_path\n",
    "    ]\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    if process.returncode != 0:\n",
    "        print(stderr)\n",
    "        raise RuntimeError(stderr)\n",
    "\n",
    "    mp4 = open(mp4_path,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display.display( display.HTML(f'<video controls loop><source src=\"{data_url}\" type=\"video/mp4\"></video>') )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
